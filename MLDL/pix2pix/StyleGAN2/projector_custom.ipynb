{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"11ElUqQ2hC5Z2okciHkr1-W3R2P6JTZ9O","authorship_tag":"ABX9TyNNXNnSjVMfEC8qPnP39Lur"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6jtQJ0KzVrRj"},"outputs":[],"source":["import copy\n","import os\n","from time import perf_counter\n","\n","import click #command line interface\n","import imageio\n","import numpy as np\n","import PIL.Image\n","import torch\n","import torch.nn.functional as F ##torch.nn와 결과는 똑같은데 torch.nn은 class instance를 활용할 수 있고 functional은 함수로 instance가 없다\n","\n","import dnnlib\n","import legacy\n","import pickle\n","\n","def project(\n","    G,\n","    target: torch.Tensor, # [C,H,W] and dynamic range [0,255], W & H must match G output resolution --> 1024*1024\n","    *,\n","    num_steps                  = 1000, #epoch 1000번\n","    w_avg_samples              = 10000,\n","    initial_learning_rate      = 0.1,\n","    initial_noise_factor       = 0.05,\n","    lr_rampdown_length         = 0.25,\n","    lr_rampup_length           = 0.05,\n","    noise_ramp_length          = 0.75,\n","    regularize_noise_weight    = 1e5,\n","    verbose                    = False,\n","    device: torch.device,\n","    extend: bool\n","):\n","    assert target.shape == (G.img_channels, G.img_resolution, G.img_resolution) ## False->error (guarantee true)\n","\n","    def logprint(*args):\n","        if verbose:\n","            print(*args)\n","\n","    G = copy.deepcopy(G).eval().requires_grad_(False).to(device) # G deepcopy & to eval mode\n","\n","    # Compute w stats(use several random z --> to genralize any Generate model)\n","    logprint(f'Computing W midpoint and stddev using {w_avg_samples} samples...')\n","    z_samples = np.random.RandomState(123).randn(w_avg_samples, G.z_dim)\n","    w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n","\n","    if extend:  #18개 vector 모두 쓰면 Visual fidelity is higher, but semantic fidelity is lower.\n","      w_samples = w_samples.cpu().numpy().astype(np.float32)\n","    else:\n","      w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)     # [N, 1, C]\n","    \n","    w_avg = np.mean(w_samples, axis=0, keepdims=True)      # [1, 1, C] or [1,18,C] #keepdims : 차원 유지\n","    w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n","\n","    # Setup noise inputs.\n","    noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n","\n","    # Load VGG16 feature detector.\n","    vgg_path = '/content/drive/MyDrive/StyleGAN2/StyleGAN2-ada_Toonify/pretrained/vgg16.pt'\n","    vgg16 = torch.jit.load(vgg_path).eval().to(device) #jit.load&sve -> weights + model_architecture\n","\n","    # Features for target image.\n","    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n","\n","    if target_images.shape[2] > 256:\n","        target_images = F.interpolate(target_images, size=(256, 256), mode='area') #resize to 256*256 because of vgg input size\n","    target_features = vgg16(target_images, resize_images=False, return_lpips=True)\n","\n","    w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True)  #[1,1,512]\n","    w_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device) #[epoch(num_steps), 1, 512]\n","    optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate) #optimizer에서 param을 그룹별로 나눔\n","\n","    # Init noise.\n","    for buf in noise_bufs.values(): #dict type -> name:buf #pgGAN block마다 다른 noise\n","        buf[:] = torch.randn_like(buf)\n","        buf.requires_grad = True\n","\n","    for step in range(num_steps):\n","        # Learning rate schedule.\n","        t = step / num_steps    #from 0 to 1 increase\n","        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2 #noise getting smaller\n","\n","        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n","        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n","        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n","        lr = initial_learning_rate * lr_ramp\n","        for param_group in optimizer.param_groups: #parameter들 그룹화해서 그룹별로 다른 lr나 최적화기법 사용 가능하도록 함\n","            param_group['lr'] = lr\n","\n","        # Synth images from opt_w.\n","        w_noise = torch.randn_like(w_opt) * w_noise_scale\n","        if extend:\n","          ws = (w_opt + w_noise)\n","        else:\n","          ws = (w_opt + w_noise).repeat([1, G.mapping.num_ws, 1])  ##num_ws=18\n","\n","        synth_images = G.synthesis(ws, noise_mode='const')\n","\n","        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n","        synth_images = (synth_images + 1) * (255/2)\n","        if synth_images.shape[2] > 256:\n","            synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n","\n","        # Features for synth images.\n","        synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)\n","        dist = (target_features - synth_features).square().sum()  ##target의 vgg output과 w_opt의 vgg output 간의 SSE를 이용\n","\n","        # Noise regularization.\n","        reg_loss = 0.0\n","        for v in noise_bufs.values():\n","            noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n","            while True:\n","                reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n","                reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n","                if noise.shape[2] <= 8:\n","                    break\n","                noise = F.avg_pool2d(noise, kernel_size=2)\n","        loss = dist + reg_loss * regularize_noise_weight\n","\n","        # Step\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","        logprint(f'step {step+1:>4d}/{num_steps}: dist {dist:<4.2f} loss {float(loss):<5.2f}')\n","\n","        # Save projected W for each optimization step.\n","        w_out[step] = w_opt.detach()[0]\n","\n","        # Normalize noise.\n","        with torch.no_grad():\n","            for buf in noise_bufs.values():\n","                buf -= buf.mean()\n","                buf *= buf.square().mean().rsqrt()\n","\n","    if extend:\n","      ret = w_out\n","    else:\n","      ret = w_out.repeat([1, G.mapping.num_ws, 1])\n","\n","    return ret\n","\n","#----------------------------------------------------------------------------\n","\n","@click.command()\n","@click.option('--network', 'network_pkl', help='Network pickle filename', required=True)\n","@click.option('--target', 'target_fname', help='Target image file to project to', required=True, metavar='FILE')\n","@click.option('--num-steps',              help='Number of optimization steps', type=int, default=1000, show_default=True)\n","@click.option('--seed',                   help='Random seed', type=int, default=303, show_default=True)\n","@click.option('--save-video',             help='Save an mp4 video of optimization progress', type=bool, default=True, show_default=True)\n","@click.option('--outdir',                 help='Where to save the output images', required=True, metavar='DIR')\n","@click.option('--verbose',                help='verbose', type=bool, default=True)\n","@click.option('--extend',                help='Want to project with extended w(18vectors)', type=bool, default=False)\n","\n","def run_projection(\n","    network_pkl: str,\n","    target_fname: str,\n","    outdir: str,\n","    save_video: bool,\n","    seed: int,\n","    num_steps: int,\n","    verbose:bool,\n","    extend: bool\n","):\n","\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","    # Load networks.\n","    print('Loading networks from \"%s\"...' % network_pkl)\n","    device = torch.device('cuda')\n","\n","    with open(network_pkl, 'rb') as f:\n","      G = pickle.load(f)['G_ema'].requires_grad_(False).to(device)\n","\n","    #with dnnlib.util.open_url(network_pkl) as fp:\n","    #    G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device) # type: ignore\n","\n","    # Load target image.\n","    target_pil = PIL.Image.open(target_fname).convert('RGB')\n","    w, h = target_pil.size\n","    s = min(w, h)\n","    target_pil = target_pil.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2)) ##align했는데 crop?\n","    target_pil = target_pil.resize((G.img_resolution, G.img_resolution), PIL.Image.LANCZOS)\n","    target_uint8 = np.array(target_pil, dtype=np.uint8)\n","\n","    # Optimize projection.\n","    start_time = perf_counter() #코드 실행시간 측정\n","    projected_w_steps = project(\n","        G,\n","        target=torch.tensor(target_uint8.transpose([2, 0, 1]), device=device), # pylint: disable=not-callable\n","        num_steps=num_steps,\n","        device=device,\n","        verbose=verbose,\n","        extend=extend\n","    )\n","    print (f'Elapsed: {(perf_counter()-start_time):.1f} s')\n","\n","\n","    if extend:\n","      n=18\n","    else:\n","      n=1\n","    # Render debug output: optional video and projected image and W vector.\n","    os.makedirs(outdir, exist_ok=True)\n","    if save_video:\n","        video = imageio.get_writer(f'{outdir}/proj_w{n}_{num_steps}.mp4', mode='I', fps=10, codec='libx264', bitrate='16M')\n","        print (f'Saving optimization progress video \"{outdir}/proj.mp4\"')\n","        for projected_w in projected_w_steps:\n","            print(projected_w.shape)\n","            synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n","            synth_image = (synth_image + 1) * (255/2)\n","            synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n","            video.append_data(np.concatenate([target_uint8, synth_image], axis=1))\n","        video.close()\n","\n","    # Save final projected frame and W vector.\n","    target_pil.save(f'{outdir}/target.png')\n","    projected_w = projected_w_steps[-1]\n","    synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n","    synth_image = (synth_image + 1) * (255/2)\n","    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n","    PIL.Image.fromarray(synth_image, 'RGB').save(f'{outdir}/proj_w{n}_{num_steps}.png')\n","    np.savez(f'{outdir}/projected_w{n}_{num_steps}.npz', w=projected_w.unsqueeze(0).cpu().numpy())\n","\n","#----------------------------------------------------------------------------\n","\n","if __name__ == \"__main__\":\n","    run_projection() # pylint: disable=no-value-for-parameter\n","\n","#----------------------------------------------------------------------------"]}]}